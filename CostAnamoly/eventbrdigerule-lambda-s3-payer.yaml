AWSTemplateFormatVersion: '2010-09-09'
Description: Cost Anomaly Detection Lambda storing data into S3 for use by
  Glue/Athena in another account

Parameters:
  S3BucketName:
    Type: String
    Description: Bucket to store cost anomaly data

Resources:
  CostAnomalySchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      Name: cost-anomaly-detection-daily-rule
      Description: Triggers Lambda to fetch cost anomaly data daily
      ScheduleExpression: rate(10 minutes)
      State: ENABLED
      Targets:
        - Arn: !GetAtt CostAnomalyLambda.Arn
          Id: CostAnomalyLambdaTarget
  AllowEventBridgeInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt CostAnomalyLambda.Arn
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt CostAnomalySchedulerRule.Arn
  CostAnomalyBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref S3BucketName
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  CostAnomalyLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: CostAnomalyLambdaRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}
                  - !Sub arn:aws:s3:::${S3BucketName}/*
              - Effect: Allow
                Action: logs:*
                Resource: '*'
              - Effect: Allow
                Action: ce:GetAnomalies
                Resource: '*'
              - Effect: Allow
                Action:
                  - glue:StartCrawler
                Resource: '*'

  CostAnomalyLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: CostAnomalyDetectionLambda
      Runtime: python3.10
      Handler: index.lambda_handler
      Timeout: 60
      Role: !GetAtt CostAnomalyLambdaRole.Arn
      MemorySize: 256
      Environment:
        Variables:
          S3_BUCKET: !Ref S3BucketName
          PREFIX: cost-anomalies
          CRAWLER_NAME: '' # Set if calling Glue crawler in another account
          ManagementAccountId: !Ref AWS::AccountId
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime
          import boto3
          from boto3.s3.transfer import S3Transfer

          BUCKET = os.environ["S3_BUCKET"]
          PREFIX = os.environ["PREFIX"]
          CRAWLER_NAME = os.environ.get("CRAWLER_NAME", "")
          MANAGEMENT_ACCOUNT_ID = os.environ["ManagementAccountId"]

          def start_crawler():
              if not CRAWLER_NAME:
                  return
              glue = boto3.client("glue")
              try:
                  glue.start_crawler(Name=CRAWLER_NAME)
              except Exception as exc:
                  logging.warning(exc)

          def store_data_to_s3(flattened_data, path):
              today = date.today()
              year = today.year
              month = today.strftime("%m")
              day = today.day
              local_file = "/tmp/tmp.json"
              with open(local_file, "w") as f:
                  f.write("\n".join([json.dumps(result) for result in flattened_data]))

              if os.path.getsize(local_file) == 0:
                  print(f"No data in file for {path}")
                  return

              s3client = boto3.client("s3")
              key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
              print(f"Uploading file {local_file} to {BUCKET}/{key}")
              S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={
                  "ACL": "bucket-owner-full-control"
              })
              print("file upload successful")

          def get_ce_costanomaly(ce, start_date, end_date):
              results = []
              next_token = None
              while True:
                  params = {
                      "DateInterval": {"StartDate": str(start_date), "EndDate": str(end_date)},
                      "MaxResults": 100
                  }
                  if next_token:
                      params["NextPageToken"] = next_token
                  response = ce.get_anomalies(**params)
                  results += response["Anomalies"]
                  next_token = response.get("NextPageToken")
                  if not next_token:
                      break
              return results

          def flatten_results(results):
              flattened_results = []
              for anomaly in results:
                  root = anomaly["RootCauses"][0] if anomaly.get("RootCauses") else {}
                  flattened_anomaly = {
                      "AnomalyId": anomaly["AnomalyId"],
                      "AnomalyStartDate": anomaly["AnomalyStartDate"],
                      "AnomalyEndDate": anomaly["AnomalyEndDate"],
                      "DimensionValue": anomaly.get("DimensionValue"),
                      "MaxImpact": anomaly["Impact"]["MaxImpact"],
                      "TotalActualSpend": anomaly["Impact"]["TotalActualSpend"],
                      "TotalExpectedSpend": anomaly["Impact"]["TotalExpectedSpend"],
                      "TotalImpact": anomaly["Impact"]["TotalImpact"],
                      "TotalImpactPercentage": anomaly["Impact"].get("TotalImpactPercentage", 0),
                      "MonitorArn": anomaly["MonitorArn"],
                      "LinkedAccount": root.get("LinkedAccount"),
                      "LinkedAccountName": root.get("LinkedAccountName"),
                      "Region": root.get("Region"),
                      "Service": root.get("Service"),
                      "UsageType": root.get("UsageType"),
                  }
                  flattened_results.append(flattened_anomaly)
              return flattened_results

          def calculate_dates(s3_path):
              end_date = datetime.now().date()
              start_date = end_date - timedelta(days=90)

              paginator = boto3.client("s3").get_paginator("list_objects_v2")
              contents = sum([
                  page.get("Contents", [])
                  for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)
              ], [])

              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj["LastModified"].date() for obj in contents]
              dates_within_90 = [d for d in last_modified_dates if d >= datetime.now().date() - timedelta(days=90)]
              return max(dates_within_90) if dates_within_90 else None

          def lambda_handler(event, context):
              start_date, end_date = calculate_dates(f"{PREFIX}/cost-anomaly-data/")
              print(f"Start: {start_date}, End: {end_date}")
              ce = boto3.client("ce")
              data = get_ce_costanomaly(ce, start_date, end_date)
              flattened = flatten_results(data)
              store_data_to_s3(flattened, f"{PREFIX}/cost-anomaly-data/management_id={MANAGEMENT_ACCOUNT_ID}")
              if flattened:
                  start_crawler()
              return {"status": "success", "records": len(flattened)}
